{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c404fb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f3099b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktonen version: 0.11.0\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "print(\"tiktonen version:\", version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d575fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4eae475b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 466, 345, 588, 8887, 30, 309, 4669, 157, 119, 223, 77, 1380, 157, 118, 94, 76, 845, 7832, 340, 13, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 286, 617, 20562, 13]\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, do you like tea? Tuyền Phạm very likes it. <|endoftext|> In the sunlit terraces of some palace.\"\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23879587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? Tuyền Phạm very likes it. <|endoftext|> In the sunlit terraces of some palace.\n"
     ]
    }
   ],
   "source": [
    "strings = tokenizer.decode(integers)\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "35fbf7ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33901, 86, 343, 86, 220, 959]\n"
     ]
    }
   ],
   "source": [
    "text = \"Akwirw ier\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "61eac30f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33901: Ak\n",
      "86: w\n",
      "343: ir\n",
      "86: w\n",
      "220:  \n",
      "959: ier\n"
     ]
    }
   ],
   "source": [
    "for id in ids:\n",
    "    print(f\"{id}: {tokenizer.decode([id])}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
