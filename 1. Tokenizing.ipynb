{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2099eae9",
   "metadata": {},
   "source": [
    "## `1. Introduction`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f639bd5",
   "metadata": {},
   "source": [
    "#### Text -> Word (Tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6cd9bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of character: 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "print(\"Total number of character:\", len(raw_text))\n",
    "print(raw_text[:99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0843c15e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4649\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(len(preprocessed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "343e5ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[')', '\"', 'The', 'height', 'of', 'his', 'glory', '\"', '--', 'that', 'was', 'what', 'the', 'women', 'called', 'it', '.', 'I', 'can', 'hear', 'Mrs', '.', 'Gideon', 'Thwing', '--', 'his', 'last', 'Chicago', 'sitter', '--']\n"
     ]
    }
   ],
   "source": [
    "print(preprocessed[70:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c19048",
   "metadata": {},
   "source": [
    "#### Word -> Token IDs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64de59b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1159\n"
     ]
    }
   ],
   "source": [
    "# Create vocabulary\n",
    "all_words = sorted(list(set(preprocessed)))\n",
    "vocab_size = len(all_words)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b152497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('--', 6)\n"
     ]
    }
   ],
   "source": [
    "vocab = {token: index for index, token in enumerate(all_words)}\n",
    "\n",
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i > 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1afe5ef",
   "metadata": {},
   "source": [
    "## `2. Implementing a simple tokenizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c5e2b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {index: text for text, index in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        ids = [self.str_to_int[item] for item in preprocessed]\n",
    "\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[id] for id in ids])\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0d435a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[58, 2, 872, 1013, 615, 541, 763, 5, 1155, 608, 5, 1, 69, 7, 39, 873, 1136, 7]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "text = \"\"\"It's the last he painted, you know,\" Mrs. Gisburn said with.\"\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "db4851b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It ' s the last he painted , you know , \" Mrs . Gisburn said with .\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ff5f0d",
   "metadata": {},
   "source": [
    "Giờ ta sẽ thử với một đoạn text mà không có trong tập training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "78f1f3cc",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Hello'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m text = \u001b[33m\"\u001b[39m\u001b[33mHello, do you like tea?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mSimpleTokenizerV1.encode\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m      9\u001b[39m preprocessed = re.split(\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33m([,.?_!\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m()\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[33m]|--|\u001b[39m\u001b[33m\\\u001b[39m\u001b[33ms)\u001b[39m\u001b[33m'\u001b[39m, text)\n\u001b[32m     10\u001b[39m preprocessed = [item.strip() \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m preprocessed \u001b[38;5;28;01mif\u001b[39;00m item.strip()]\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m ids = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstr_to_int\u001b[49m\u001b[43m[\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m preprocessed]\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ids\n",
      "\u001b[31mKeyError\u001b[39m: 'Hello'"
     ]
    }
   ],
   "source": [
    "text = \"Hello, do you like tea?\"\n",
    "tokenizer.encode(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f38f50",
   "metadata": {},
   "source": [
    "- Vấn đề là từ \"Hello\" không nằm trong *The Verdict* nên sẽ không có trong *vocabulary*. Điều này cho thấy cần phải chuẩn bị 1 tập training lớn và đa dạng hơn để mở rộng cho *vocabulary*.\n",
    "\n",
    "- Ở phần sau, ta sẽ tiếp tục thử nghiệm Tokenizer trên văn bản chứa những từ mà *vocabulary* chưa có, đồng thời thảo luận về các `token đặc biệt` bổ sung có thể được sử dụng để cung cấp thêm ngữ cảnh cho LLM trong quá trình training. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e17fab3",
   "metadata": {},
   "source": [
    "## `3. Adding special context tokens`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3fa9bc61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1161\n"
     ]
    }
   ],
   "source": [
    "all_tokens = sorted(list(set(preprocessed)))        # List of all unique tokens\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "vocab = {token: index for index, token in enumerate(all_tokens)}\n",
    "\n",
    "print(len(vocab.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "68f16fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('younger', 1156)\n",
      "('your', 1157)\n",
      "('yourself', 1158)\n",
      "('<|endoftext|>', 1159)\n",
      "('<|unk|>', 1160)\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(list(vocab.items())[-5:]):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd138e9c",
   "metadata": {},
   "source": [
    "`A simple text tokenizer that handles unknown words`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "333bea96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {index: text for text, index in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        preprocessed = [item if item in self.str_to_int else \"<|unk|>\" for item in preprocessed]\n",
    "        ids = [self.str_to_int[item] for item in preprocessed]\n",
    "\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[id] for id in ids])\n",
    "\n",
    "        # Remove space before punctuation\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "78e526d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n"
     ]
    }
   ],
   "source": [
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "debbde29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1160, 5, 362, 1155, 642, 1000, 10, 1159, 57, 1013, 981, 1009, 738, 1013, 1160, 7]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "print(tokenizer.encode(text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3eeb36f",
   "metadata": {},
   "source": [
    "Có thể thấy ở trên có ID 1160 cho token `<|unk|>` và 1159 cho token `<|endoftext|>`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452ca6e7",
   "metadata": {},
   "source": [
    "Giờ hãy *decode* để xem lại text gốc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8ee8f911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(tokenizer.encode(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "407845cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests, csv, io\n",
    "# import re\n",
    "\n",
    "# url = \"https://docs.google.com/spreadsheets/d/1BZqhx0lXy107MasuLkLdjXaR319WWKXz9GLGjWg_60Y/export?format=xlsx\"\n",
    "# resp = requests.get(url, timeout=60)\n",
    "# excel_content = resp.content\n",
    "\n",
    "# with open(\"complete_workbook.xlsx\", \"wb\") as f:\n",
    "#     f.write(excel_content) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d1840e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    STT GIÁO VIÊN Nơi ở (Quận cũ) 2- MON - 22/09_Sáng 2- MON - 22/09_Chiều  \\\n",
      "20   21   Thảo Ly         Thủ Đức                 NaN       TiH Từ Đức (2)   \n",
      "\n",
      "    3- TUE - 23/09_Sáng                       3- TUE - 23/09_Chiều  \\\n",
      "20  THCS Bình Chiểu (3)  Chạy trường  An Bình (1) -  Bình Quới (1)   \n",
      "\n",
      "   4- WED - 24/09_Sáng 4- WED - 24/09_Chiều 5- THURS - 25/09_Sáng  \\\n",
      "20                 NaN       TiH Từ Đức (3)                   NaN   \n",
      "\n",
      "   5- THURS - 25/09_Chiều 6- FRI - 26/09_Sáng 6- FRI - 26/09_Chiều  \\\n",
      "20         TiH Từ Đức (3)                 NaN       TiH Từ Đức (3)   \n",
      "\n",
      "   7- SAT - 27/09_Sáng  Tổng số tiết trong tuần_Tiểu học  \\\n",
      "20           LV ONLINE                               NaN   \n",
      "\n",
      "    Tổng số tiết trong tuần_THCS  Tổng số tiết trong tuần_THPT  \n",
      "20                           NaN                           NaN  \n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# df = pd.read_excel(\"complete_workbook.xlsx\", sheet_name=\"229 - 279\", skiprows=2, header=[0, 1])\n",
    "\n",
    "# # Làm sạch level 2: đổi 'Unnamed: ...' -> \"\" (trống)\n",
    "# lvl0 = df.columns.get_level_values(0).astype(str)\n",
    "# lvl1 = df.columns.get_level_values(1).astype(str).map(\n",
    "#     lambda s: \"\" if s.startswith(\"Unnamed:\") else s\n",
    "# )\n",
    "\n",
    "# # Gộp 2 level thành 1 tên cột phẳng\n",
    "# new_cols = []\n",
    "# for a, b in zip(lvl0, lvl1):\n",
    "#     a = re.sub(r\"\\s+\", \" \", a).strip()          # bỏ xuống dòng/thừa khoảng trắng\n",
    "#     b = re.sub(r\"\\s+\", \" \", b).strip()\n",
    "#     new_cols.append(a if b == \"\" else f\"{a}_{b}\")\n",
    "\n",
    "# df.columns = new_cols\n",
    "\n",
    "# # (Tuỳ chọn) xử lý trùng tên cột nếu có\n",
    "# df = df.loc[:, ~df.columns.duplicated()]\n",
    "\n",
    "# # ---- Lọc GIÁO VIÊN = 'Thảo Ly' (không phân biệt hoa thường, bỏ khoảng trắng thừa) ----\n",
    "# mask = df['GIÁO VIÊN'].astype(str).str.strip().str.casefold() == 'Thảo Ly'.casefold()\n",
    "# filtered = df[mask]\n",
    "\n",
    "# print(filtered.head())\n",
    "\n",
    "# # Nếu bạn muốn lấy dạng \"tên_cột - value\" cho từng dòng:\n",
    "# result = [\n",
    "#     [f\"{col} - {row[col]}\" for col in filtered.columns]\n",
    "#     for _, row in filtered.iterrows()\n",
    "# ]\n",
    "# # print(df.head())\n",
    "\n",
    "# # df[df['GIÁO VIÊN'] == 'Thảo Ly']\n",
    "# # print(df.columns.tolist())\n",
    "\n",
    "# # filtered = df[df['GIÁO VIÊN'] == 'THẢO LY']\n",
    "\n",
    "# # result = []\n",
    "# # for _, row in filtered.iterrows():\n",
    "# #     for col, val in row.items():\n",
    "# #         result.append(f\"{col} - {val}\")\n",
    "\n",
    "# # print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
