{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94c169f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd012303",
   "metadata": {},
   "source": [
    "## 1. GPT Backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9cb26f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257, # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"emb_dim\": 768, # Embedding dimension\n",
    "    \"n_heads\": 12, # Number of attention heads\n",
    "    \"n_layers\": 12, # Number of layers\n",
    "    \"drop_rate\": 0.1, # Dropout rate\n",
    "    \"qkv_bias\": False # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fc0543",
   "metadata": {},
   "source": [
    "- Các thành phần của _DummyGPT_ này chúng ta đều đã tìm hiểu trong các chương trước, với _DummyTransformerBlock_ là 1 khối Transformer & _DummyLayerNorm_ là 1 lớp Layer Normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d58b470d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyGPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[DummyTransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        self.final_norm = DummyLayerNorm(cfg[\"emb_dim\"]) #B\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "\n",
    "class DummyTransformerBlock(nn.Module): #C\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x): #D\n",
    "        return x\n",
    "    \n",
    "class DummyLayerNorm(nn.Module): #E\n",
    "    def __init__(self, normalized_shape, eps=1e-5): #F\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86092c86",
   "metadata": {},
   "source": [
    "## 2. Layer Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876b9709",
   "metadata": {},
   "source": [
    "- Khởi tạo random 2 vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "40547e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1115,  0.1204, -0.3696, -0.2404, -1.1969],\n",
      "        [ 0.2093, -0.9724, -0.7550,  0.3239, -0.1085]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "batch = torch.randn(2, 5)\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e297c1bf",
   "metadata": {},
   "source": [
    "- Sau khi đưa qua 1 lớp _Linear_ và qua hàm kích hoạt _ReLU_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6394b2b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2260, 0.3470, 0.0000, 0.2216, 0.0000, 0.0000],\n",
      "        [0.2133, 0.2394, 0.0000, 0.5198, 0.3297, 0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "layer = nn.Sequential(\n",
    "    nn.Linear(5, 6),\n",
    "    nn.ReLU(),\n",
    ")\n",
    "outputs = layer(batch)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83ac7d1",
   "metadata": {},
   "source": [
    "- Lớp mạng nơ-ron vừa code ở trên bao gồm _1 lớp tuyến tính_ `Linear` đi kèm với _1 hàm kích hoạt_ `ReLU` (Rectified Linear Unit).\n",
    "\n",
    "- Hàm `ReLU` đơn giản là sẽ chặn các giá trị đầu vào âm và chuyển chúng về 0, đảm bảo rằng đầu ra của lớp chỉ chứa các giá trị dương."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60cdb84d",
   "metadata": {},
   "source": [
    "- Trước khi áp dụng `Layer Normalization` cho các outputs này, cùng tính `mean` & `variance` của các outputs này trước:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1ec4e68c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[0.1324],\n",
      "        [0.2170]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[0.0231],\n",
      "        [0.0398]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mean = outputs.mean(dim=-1, keepdim=True)\n",
    "variance = outputs.var(dim=-1, keepdim=True)\n",
    "print(f\"Mean:\\n {mean}\")\n",
    "print(f\"Variance:\\n {variance}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6da25cf",
   "metadata": {},
   "source": [
    "- Việc sử dụng `keepdim=True` trong các phép toán như `mean` & `variance` giúp chúng ta giữ nguyên số chiều như tensor đầu vào. Ví dụ nếu không có `keepdim=True`, thì _mean_ trả về sẽ là 1 tensor 2 chiều _[0.1324, 0.2170]_ thay vì 1 ma trận 2x1 _[[0.1324], [0.2170]]_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22480dd8",
   "metadata": {},
   "source": [
    "- Tiếp theo đây là đoạn code triển khai _layer normalization_, với công thức sử dụng của [Standardization](https://tyanfarm.github.io/tyan-blog/docs/build-llm-from-scratch/bonus-section/normalization#standardization-z-score-normalization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "532ac711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized outputs:\n",
      " tensor([[ 0.6159,  1.4126, -0.8719,  0.5872, -0.8719, -0.8719],\n",
      "        [-0.0189,  0.1121, -1.0876,  1.5173,  0.5647, -1.0876]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "Mean:\n",
      " tensor([[9.9341e-09],\n",
      "        [0.0000e+00]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "outputs_norm = (outputs - mean) / torch.sqrt(variance)\n",
    "mean = outputs_norm.mean(dim=-1, keepdim=True)\n",
    "variance = outputs_norm.var(dim=-1, keepdim=True)\n",
    "print(f\"Normalized outputs:\\n {outputs_norm}\")\n",
    "print(f\"Mean:\\n {mean}\")\n",
    "print(f\"Variance:\\n {variance}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942cd244",
   "metadata": {},
   "source": [
    "- Giá trị `9.9341e-09` là 9.9341 * 10^-9 = 0.0000000099341. Giá trị này gần 0 nhưng không hoàn toàn bằng 0.\n",
    "\n",
    "- Vì vậy để dễ hiển thị hơn, ta sử dụng `torch.set_printoptions(sci_mode=False)` để tắt chế độ số học."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4670f2d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized outputs:\n",
      " tensor([[ 0.6159,  1.4126, -0.8719,  0.5872, -0.8719, -0.8719],\n",
      "        [-0.0189,  0.1121, -1.0876,  1.5173,  0.5647, -1.0876]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "Mean:\n",
      " tensor([[0.0000],\n",
      "        [0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.set_printoptions(sci_mode=False)\n",
    "print(f\"Normalized outputs:\\n {outputs_norm}\")\n",
    "print(f\"Mean:\\n {mean}\")\n",
    "print(f\"Variance:\\n {variance}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f4c0e1",
   "metadata": {},
   "source": [
    "- Giờ thì ta đóng gói lại thành 1 class `LayerNorm` hoàn chỉnh:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "01b74ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(sci_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a1930462",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224f6bd9",
   "metadata": {},
   "source": [
    "- `unbiased=False`: với hàm _var()_ thì nó chia cho _n-1_ vì tuân theo _Phương sai mẫu (Sample Variance)_. Ở _layer normalization_ này ta muốn tính _Phương sai tổng thể (Population Variance)_ nên ta set __unbiased=False__ để chia cho _n_.\n",
    "\n",
    "- `self.eps`: _epsilon_ ngăn chặn lỗi chia cho 0 khi _phương sai_ cực nhỏ.\n",
    "\n",
    "- `self.scale` & `self.shift`: _scale_ ($\\gamma$) và _shift_ ($\\beta$) là hai tham số để điều chỉnh giá trị của _layer normalization_.\n",
    "    + Khi bạn thực hiện chuẩn hóa (trừ mean, chia std), bạn đang ép dữ liệu về trạng thái: Trung bình = 0 và Độ lệch chuẩn = 1.\n",
    "\n",
    "    + Tuy nhiên, trong một số trường hợp, mạng nơ-ron có thể \"nhận ra\" rằng việc giữ dữ liệu ở trạng thái (0, 1) này lại làm giảm khả năng học tập của nó. Vì vậy, người ta thêm vào hai tham số:\n",
    "\n",
    "        + Scale ($\\gamma$): Cho phép mô hình thay đổi độ lệch chuẩn (độ co giãn của dữ liệu).\n",
    "\n",
    "        + Shift ($\\beta$): Cho phép mô hình dịch chuyển giá trị trung bình sang một con số khác 0.\n",
    "\n",
    "    + Vì vậy nên khởi tạo _scale_ và _shift_ là 2 tensor _1_ và _0_ với _dim_ là số chiều của _input_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cb04f650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Layer Normalization:\n",
      " tensor([[ 0.5528,  1.0693, -0.0223,  0.2656, -1.8654],\n",
      "        [ 0.9087, -1.3767, -0.9564,  1.1304,  0.2940]], grad_fn=<AddBackward0>)\n",
      "Mean:\n",
      " tensor([[-0.0000],\n",
      "        [ 0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "layer_norm = LayerNorm(emb_dim=5)\n",
    "out_layer_norm = layer_norm(batch)\n",
    "mean = out_layer_norm.mean(dim=-1, keepdim=True)\n",
    "var = out_layer_norm.var(dim=-1, unbiased=False, keepdim=True)\n",
    "print(f\"Output Layer Normalization:\\n {out_layer_norm}\")\n",
    "print(f\"Mean:\\n {mean}\")\n",
    "print(f\"Variance:\\n {var}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
